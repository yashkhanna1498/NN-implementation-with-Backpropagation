{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This code has been written for AND Gate with two input 0 and 1\n",
    "We will proceed in the following manner  : <br>\n",
    "*Code for Forward propagation(0 hidden layer) <br>\n",
    "*Code for Forward propagation(1 hidden layer)<br>\n",
    "*Code for Neural Network with backpropagation (0 hidden layer)<br>\n",
    "*Code for XOR Gate Neural Network with backpropagation (1 hidden layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward propagation without any hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#implementing AND function\n",
    "X = np.array([[0,0], [0,1], [1,0], [1,1]])  \n",
    "Y = np.array([[0,0,0,1]]).T\n",
    "print(\"shape of x and y are :\",X.shape, Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sig(z):  #z=wx+b\n",
    "    return 1/(1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of x and y are : (4, 2) (4, 1)\n",
      "initial random wts and bias are :  [[0.62296978]\n",
      " [0.17629434]]  , [-0.03044741]\n",
      "\n",
      "z for input layer is : [[-0.03044741]\n",
      " [ 0.14584692]\n",
      " [ 0.59252237]\n",
      " [ 0.7688167 ]]\n",
      "\n",
      "output is [[0.49238873]\n",
      " [0.53639724]\n",
      " [0.64394368]\n",
      " [0.68326487]]\n"
     ]
    }
   ],
   "source": [
    "#  output layer weights (as there is no hidden layer)\n",
    "# np.random.random will give random number betwen 0 to 1\n",
    "#so we'll be multiply it by 2, so range is now (0,2) and then subtract 1 to make range -1 to 1\n",
    "weights = 2* np.random.random((2, 1)) - 1  #(2,1) is shape of array\n",
    "bias = 2 * np.random.random(1) - 1\n",
    "#here we have taken no of bias =1 as we're without any hidden layer and output layer has 1 unit\n",
    "# but no of bias in each layer depends on the no of units in next consecutive layer\n",
    "print(\"initial random wts and bias are : \",weights,\" ,\", bias)\n",
    "# forward propagation without any hidden layer\n",
    "output0 = X  #output of 1st layer that is our output layer in this case\n",
    "#output0 is output of input layer\n",
    "z0=np.dot(output0, weights) + bias\n",
    "print()\n",
    "print(\"z for input layer is :\",z0)\n",
    "output = sig(z0) \n",
    "print()\n",
    "print(\"output is\" ,output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward propagation with 1 hidden layer\n",
    "taking 2 units in hidden layer and 2 units in input layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.53184492],\n",
       "       [0.52157192],\n",
       "       [0.51263876],\n",
       "       [0.50518544]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([[0,0], [0,1], [1,0], [1,1]])  #implementing AND function\n",
    "Y = np.array([[0,0,0,1]]).T\n",
    "\n",
    "def sig(z): \n",
    "    return 1/(1 + np.exp(-z))\n",
    "\n",
    "# one hidden layer weights and bias\n",
    "wh = 2* np.random.random((2, 2)) - 1  #4wts required for output of input layer\n",
    "bh = 2* np.random.random((1, 2)) - 1  \n",
    "#output layer hidden weights and bias\n",
    "wo = 2 * np.random.random((2, 1)) - 1 #2 wts required for output of hidden layer \n",
    "bo = 2 * np.random.random((1,1)) - 1\n",
    "\n",
    "# forward propagation with one hidden layer\n",
    "output0 = X\n",
    "outputHidden = sig(np.dot(output0, wh) + bh) \n",
    "output = sig(np.dot(outputHidden, wo) + bo)\n",
    "output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Derivative of sigmoid function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivativeSig(z):\n",
    "    return sig(z)*(1 - sig(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network code for 0 hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward propagation without hidden layer\n",
    "import numpy as np\n",
    "\n",
    "def grad_desc(output,weights,bias,X):\n",
    "    lr=.1\n",
    "    output0 = X \n",
    "    z0=np.dot(output0, weights) + bias\n",
    "    output = sig(z0) \n",
    "    \n",
    "    first_term=output-Y  #yp-ya\n",
    "    input_for_last_layer = np.dot(output0, weights) + bias\n",
    "    second_term = derivativeSig(input_for_last_layer)\n",
    "    #we can also calculate second term by second_term=output(1-output)\n",
    "    # as we have no hidden layer oj=output\n",
    "    first_two = first_term * second_term\n",
    "    \n",
    "#     differentiation_E_wrt_W = np.array([[0.0],[0.0]]) # 2X1 dimension bcz original wt dimension is also 2,1\n",
    "\n",
    "#     for i in range(2):\n",
    "#         for j in range(4):\n",
    "#             differentiation_E_wrt_W[i][0] += first_two[j][0] * output0[j][i]\n",
    "#             #features are stored column wise in output, thats why i in 2nd place#\n",
    "##########################\n",
    "    #using matrix multiplication without for loop\n",
    "    differentiation_E_wrt_W =np.dot(output0.T, first_two)\n",
    "    weights=weights-lr*differentiation_E_wrt_W\n",
    "    bias_change = np.sum(first_two)\n",
    "##########################\n",
    "    \n",
    "#     bias_change = 0.0\n",
    "#     for j in range(4):\n",
    "#         bias_change += first_two[j][0] * 1\n",
    "    bias = bias - lr * bias_change\n",
    "    output = sig(np.dot(X, weights) + bias)\n",
    "\n",
    "    return output,weights,bias\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def run():\n",
    "    X = np.array([[0,0], [0,1], [1,0], [1,1]]) \n",
    "    Y = np.array([[0,0,0,1]]).T\n",
    "    def sig(z): \n",
    "        return 1/(1 + np.exp(-z))\n",
    "    #initial random allocation\n",
    "    weights = 2* np.random.random((2, 1)) - 1 \n",
    "    bias = 2 * np.random.random(1) - 1\n",
    "    \n",
    "    \n",
    "    output0 = X\n",
    "    output = sig(np.dot(output0, weights) + bias)\n",
    "    \n",
    "    \n",
    "    \n",
    "    for i in range(10000):\n",
    "        output,weights,bias=grad_desc(output,weights,bias,X)\n",
    "    \n",
    "    output = sig(np.dot(X, weights) + bias)\n",
    "    \n",
    "    print(\"output\",output,\"\\n bias \",bias,\"\\n weight\",weights)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output [[2.46994059e-04]\n",
      " [5.57012712e-02]\n",
      " [5.57012722e-02]\n",
      " [9.33703391e-01]] \n",
      " bias  [-8.30589925] \n",
      " weight [[5.47545967]\n",
      " [5.47545965]]\n"
     ]
    }
   ],
   "source": [
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  4,  9, 16])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=np.array([1,2,3,4])\n",
    "b=np.array([1,2,3,4])\n",
    "a*b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network code for 1 hidden layer XOR Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[0,0], [0,1], [1,0], [1,1]]) \n",
    "Y = np.array([[0,1,1,0]]).T\n",
    "\n",
    "wh = 2* np.random.random((2, 2)) - 1 \n",
    "bh = 2* np.random.random((1, 2)) - 1  \n",
    "wo = 2 * np.random.random((2, 1)) - 1 \n",
    "bo = 2 * np.random.random((1,1)) - 1\n",
    "lr=0.1\n",
    "\n",
    "\n",
    "###forward propagation###\n",
    "output0 = X\n",
    "input_hidden=np.dot(output0, wh) + bh\n",
    "outputHidden = sig(input_hidden) \n",
    "input_outputLayer=np.dot(outputHidden, wo) + bo\n",
    "output = sig(input_outputLayer)\n",
    "################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00896748],\n",
       "       [0.99067134],\n",
       "       [0.99066218],\n",
       "       [0.01145033]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(10000):\n",
    "    output0 = X\n",
    "    input_hidden=np.dot(output0, wh) + bh\n",
    "    outputHidden = sig(input_hidden) \n",
    "    input_outputLayer=np.dot(outputHidden, wo) + bo\n",
    "    output = sig(input_outputLayer)\n",
    "    \n",
    "    \n",
    "    \n",
    "    first_term_output_layer=output-Y  #yp-ya\n",
    "    second_term_output_layer = derivativeSig(input_outputLayer)\n",
    "    first_two_output_layer = first_term_output_layer * second_term_output_layer\n",
    "\n",
    "    first_term_hidden_layer= np.dot(first_two_output_layer,wo.T)\n",
    "    second_term_hidden_layer=derivativeSig(input_hidden)\n",
    "    first_two_hidden_layer=first_term_hidden_layer*second_term_hidden_layer\n",
    "\n",
    "    weight_changes_output_layer=np.dot(outputHidden.T,first_two_output_layer)\n",
    "    bias_changes_output_layer=np.sum(first_two_output_layer,axis=0,keepdims=True)\n",
    "\n",
    "    weight_changes_hidden_layer=np.dot(output0.T,first_two_hidden_layer)\n",
    "    bias_changes_hidden_layer=np.sum(first_two_hidden_layer,axis=0,keepdims=True)\n",
    "\n",
    "\n",
    "\n",
    "    wo=wo-lr*weight_changes_output_layer\n",
    "    bo=bo-lr*bias_changes_output_layer\n",
    "\n",
    "    wh=wh-lr*weight_changes_hidden_layer\n",
    "    bh=bh-lr*bias_changes_hidden_layer\n",
    "\n",
    "output0 = X\n",
    "input_hidden=np.dot(output0, wh) + bh\n",
    "outputHidden = sig(input_hidden) \n",
    "input_outputLayer=np.dot(outputHidden, wo) + bo\n",
    "output = sig(input_outputLayer)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
